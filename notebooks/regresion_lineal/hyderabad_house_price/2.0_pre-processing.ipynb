{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8964cec",
   "metadata": {},
   "source": [
    "# Hyderabad House Price Pre-procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PRICE = \"Price\"\n",
    "LOCATION = \"Location\"\n",
    "REFRIGERATOR = \"Refrigerator\"\n",
    "SECURITY = \"24X7Security\"\n",
    "AREA = \"Area\"\n",
    "BED_ROOMS = \"No. of Bedrooms\"\n",
    "\n",
    "house_prices_ds = pd.read_csv('../../../datasets/raw/hyderabad_house_price_with_nullables.csv')\n",
    "labels_ds = house_prices_ds[PRICE]\n",
    "features_ds = house_prices_ds.drop(columns=[PRICE])\n",
    "numeric_features_df = features_ds.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "categorical_features_df = features_ds[LOCATION]\n",
    "processed_ds = None\n",
    "\n",
    "def convertArrayToDataset(dataset):\n",
    "    return pd.DataFrame(\n",
    "    dataset,\n",
    "    columns=dataset.columns,\n",
    "    index=dataset.index)\n",
    "\n",
    "def exportDataset(dataset, step):\n",
    "    dataset.to_csv(f\"../../../datasets/processed/hyderabad_house_price_{step}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef606430",
   "metadata": {},
   "source": [
    "## 1.0 Missing Values\n",
    "\n",
    "Replacing missing numerical values with the most frequent values (mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a319c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class WindowImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Imputa Column usando vecinos con Label column en [Label - window, Label + window].\n",
    "    Si no hay vecinos, expande la ventana (x2) hasta max_expansions veces.\n",
    "    Fallback: mediana global de AREA.\n",
    "    \"\"\"\n",
    "    def __init__(self, column=\"Area\", label_col=\"Price\", window=1000.0,\n",
    "                 agg=\"median\", expand=True, max_expansions=2):\n",
    "        self.column = column\n",
    "        self.label_col = label_col\n",
    "        self.window = float(window)\n",
    "        self.agg = agg\n",
    "        self.expand = expand\n",
    "        self.max_expansions = int(max_expansions)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        df[self.label_col] = pd.to_numeric(df[self.label_col], errors=\"coerce\")\n",
    "        df[self.column]  = pd.to_numeric(df[self.column],  errors=\"coerce\")\n",
    "        self._train_ = df[[self.label_col, self.column]].copy()\n",
    "\n",
    "        if self.agg == \"mean\":\n",
    "            self._aggfunc_ = np.nanmean\n",
    "        else:\n",
    "            self._aggfunc_ = np.nanmedian\n",
    "\n",
    "        self.global_fallback_ = float(self._aggfunc_(self._train_[self.column].values))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df[self.label_col] = pd.to_numeric(df[self.label_col], errors=\"coerce\")\n",
    "        df[self.column]  = pd.to_numeric(df[self.column],  errors=\"coerce\")\n",
    "\n",
    "        na_idx = df[self.column].isna()\n",
    "        if not na_idx.any():\n",
    "            return df\n",
    "\n",
    "        for idx, p in df.loc[na_idx, self.label_col].items():\n",
    "            # Si no hay Price para esta fila, fallback global.\n",
    "            if pd.isna(p):\n",
    "                df.at[idx, self.column] = self.global_fallback_\n",
    "                continue\n",
    "\n",
    "            w = self.window\n",
    "            imputed = None\n",
    "            for _ in range(self.max_expansions + 1):\n",
    "                low, high = p - w, p + w\n",
    "                mask = (\n",
    "                    self._train_[self.label_col].between(low, high)\n",
    "                    & self._train_[self.column].notna()\n",
    "                )\n",
    "                candidates = self._train_.loc[mask, self.column]\n",
    "                if len(candidates):\n",
    "                    imputed = float(self._aggfunc_(candidates.values))\n",
    "                    break\n",
    "                if not self.expand:\n",
    "                    break\n",
    "                w *= 2  # expandimos la ventana\n",
    "\n",
    "            if imputed is None:\n",
    "                imputed = self.global_fallback_\n",
    "            df.at[idx, self.column] = imputed\n",
    "\n",
    "        return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = house_prices_ds.copy()\n",
    "\n",
    "# Impute the REFRIGERATOR and SECURITY with the most frequent value\n",
    "mf_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "mf_cols = [REFRIGERATOR, SECURITY]\n",
    "df[mf_cols] = mf_imputer.fit_transform(df[mf_cols])\n",
    "\n",
    "# Impute Area with the Custom Window Imputer\n",
    "area_imputer = WindowImputer(column=AREA, label_col=PRICE, window=1000, agg=\"median\")\n",
    "df = area_imputer.fit_transform(df)\n",
    "\n",
    "# Impute Bed rooms with the Custom Window Imputer\n",
    "bed_imputer = WindowImputer(column=BED_ROOMS, label_col=PRICE, window=1000, agg=\"median\")\n",
    "imputed_features_df = bed_imputer.fit_transform(df)\n",
    "\n",
    "# Write processed dataset\n",
    "processed_ds = convertArrayToDataset(imputed_features_df)\n",
    "exportDataset(processed_ds, \"1_nullables\")\n",
    "\n",
    "print(\"Total Missing values:\", processed_ds.isna().sum().sum())\n",
    "processed_ds.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b39f4c",
   "metadata": {},
   "source": [
    "## 2.0 Transform Categorical Data\n",
    "\n",
    "Convert categorical features to Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c425639",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform Categorical Data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = house_prices_ds.copy()\n",
    "\n",
    "# Create encoder\n",
    "encoder = OneHotEncoder(sparse_output=True, drop=None, handle_unknown=\"ignore\")\n",
    "\n",
    "# Set and transform Location\n",
    "location_encoded = encoder.fit_transform(df[[\"Location\"]])\n",
    "\n",
    "\n",
    "# Alternative: use dummy variables when there are few categories \n",
    "# house_prices_ds_encoded = pd.get_dummies(processed_ds, columns=[\"Location\"])\n",
    "\n",
    "categorical_features_df = pd.DataFrame(\n",
    "    location_encoded.toarray(),\n",
    "    columns=encoder.get_feature_names_out([LOCATION]),\n",
    "    index=house_prices_ds.index\n",
    ")\n",
    "\n",
    "\n",
    "processed_ds = pd.concat(\n",
    "    [processed_ds.drop(columns=[LOCATION]), categorical_features_df],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "exportDataset(processed_ds, \"2_categorical\")\n",
    "\n",
    "processed_ds.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5245ab",
   "metadata": {},
   "source": [
    "## 3.0 Fix Outiers values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class BinaryRangeImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Reemplaza valores fuera de [0,1] en columnas binarias por el valor más frecuente de esa columna.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, valid_range=(0,1)):\n",
    "        self.columns = columns\n",
    "        self.valid_range = valid_range\n",
    "        self.fill_values_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            # valores válidos\n",
    "            mask_valid = df[col].between(self.valid_range[0], self.valid_range[1])\n",
    "            mode_val = df.loc[mask_valid, col].mode(dropna=True)\n",
    "            # fallback: si no hay válidos, usa 0\n",
    "            self.fill_values_[col] = mode_val.iloc[0] if not mode_val.empty else 0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            mask_invalid = ~df[col].between(self.valid_range[0], self.valid_range[1])\n",
    "            df.loc[mask_invalid, col] = self.fill_values_[col]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [\"MaintenanceStaff\", \"Gymnasium\",\"SwimmingPool\",\"LandscapedGardens\",\"JoggingTrack\",\"RainWaterHarvesting\",\n",
    "        \"IndoorGames\",\"ShoppingMall\",\"Intercom\",\"SportsFacility\",\"ATM\",\"ClubHouse\",\"School\",\n",
    "        \"24X7Security\",\"PowerBackup\",\"CarParking\",\"StaffQuarter\",\"Cafeteria\",\"MultipurposeRoom\",\n",
    "        \"Hospital\",\"WashingMachine\",\"Gasconnection\",\"AC\",\"Wifi\",\"Children'splayarea\",\n",
    "        \"LiftAvailable\",\"BED\",\"VaastuCompliant\",\"Microwave\",\"GolfCourse\",\"TV\",\"DiningTable\",\n",
    "        \"Sofa\",\"Wardrobe\",\"Refrigerator\"]\n",
    "\n",
    "\n",
    "mask_invalid_before = (processed_ds[binary_cols] < 0) | (processed_ds[binary_cols] > 1)\n",
    "rows_invalid_before = processed_ds[mask_invalid_before.any(axis=1)]\n",
    "print(\"Invalid binary features before:\", len(rows_invalid_before[binary_cols]))\n",
    "\n",
    "# 1. Transform invalid binary values by most frecuent\n",
    "imputer = BinaryRangeImputer(columns=binary_cols, valid_range=(0,1))\n",
    "binary_impute = imputer.fit_transform(processed_ds)\n",
    "exportDataset(processed_ds, \"3_outiers\")\n",
    "\n",
    "mask_invalid_after = (binary_impute[binary_cols] < 0) | (binary_impute[binary_cols] > 1)\n",
    "rows_invalid_after = binary_impute[mask_invalid_after.any(axis=1)]\n",
    "print(\"Invalid binary features after:\", len(rows_invalid_after[binary_cols]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efca3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for col in binary_cols:\n",
    "    sns.boxplot(x=binary_impute[col])\n",
    "    plt.title(f\"Boxplot de {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebe8bf",
   "metadata": {},
   "source": [
    "## 4.0 Checking multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c02748",
   "metadata": {},
   "source": [
    "# 4.1 Variance Inflation Factor\n",
    "\n",
    "# VIF = 1 → No colinealidad.\n",
    "# VIF entre 1 y 5 → Aceptable.\n",
    "# VIF > 10 → Problema serio de multicolinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor\n",
    "\n",
    "# VIF = 1 → No colinealidad.\n",
    "# VIF entre 1 y 5 → Aceptable.\n",
    "# VIF > 10 → Problema serio de multicolinealidad.\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "df = processed_ds.copy()    \n",
    "\n",
    "# Selecciona solo las variables numéricas relevantes (sin la target)\n",
    "X = df.select_dtypes(include=[\"int64\", \"float64\"]).drop(columns=[LOCATION, PRICE], errors=\"ignore\")\n",
    "\n",
    "# Calcular VIF para cada columna\n",
    "# VIF = 1: No multicollinearity.\n",
    "# VIF between 1 and 5: Moderate multicollinearity.\n",
    "# VIF > 5 or 10: High multicollinearity (depending on the analyst's criteria).\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9170c48",
   "metadata": {},
   "source": [
    "## 4.2 Split training, and validation, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b23e0",
   "metadata": {},
   "source": [
    "## 4.2 Fixing Multicollinearity\n",
    "\n",
    "# Eliminate redundant variables (such as X2 if it is derived from X1).\n",
    "# Combine correlated variables (for example, using PCA).\n",
    "# Apply regularization (Ridge or Lasso).\n",
    "# Review the model structure and consider transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90ec74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "class ModelType(Enum):\n",
    "    PCA = \"PCA\"\n",
    "    SVD = \"SVD\"\n",
    "    LDA = \"LDA\"\n",
    "\n",
    "# Función de regresión lineal\n",
    "def lineal_regresion_model_apply(x_train, y_train, x_val, y_val):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    predicts = model.predict(x_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, predicts))\n",
    "    r2 = r2_score(y_val, predicts)\n",
    "    return rmse, r2\n",
    "    \n",
    "def evalueate_principal_components_model(model_type: ModelType, x_train, y_train, x_val, y_val, n):\n",
    "    model = None\n",
    "    \n",
    "    if model_type == ModelType.PCA:\n",
    "        model = PCA(n_components=n)\n",
    "        x_train_pca = model.fit_transform(x_train)\n",
    "        x_val_pca = model.transform(x_val)\n",
    "    elif model_type == ModelType.SVD:\n",
    "        model = TruncatedSVD(n_components=n)\n",
    "        x_train_pca = model.fit_transform(x_train)\n",
    "        x_val_pca = model.transform(x_val)\n",
    "    elif model_type == ModelType.LDA:\n",
    "        model = LinearDiscriminantAnalysis(n_components=n)\n",
    "        x_train_pca = model.fit_transform(x_train, y_train)\n",
    "        x_val_pca = model.transform(x_val)\n",
    "    else:\n",
    "        raise ValueError(\"Modelo no soportado\")\n",
    "    \n",
    "    rmse, r2 = lineal_regresion_model_apply(x_train_pca, y_train, x_val_pca, y_val)\n",
    "\n",
    "    return rmse, r2, model\n",
    "\n",
    "\n",
    "# Selección del mejor modelo PCA usando score balanceado\n",
    "def find_best_principal_components_model(model_type: ModelType, x_train, y_train, x_val, y_val, rmse_baseline, alpha=1.0, beta=1.0):\n",
    "    best_score = float(\"inf\")\n",
    "    best_n = None\n",
    "    best_pca_model = None\n",
    "    best_rmse = None\n",
    "    best_r2 = None\n",
    "    max_components = x_train.shape[1]\n",
    "\n",
    "    print(\"\\nFinding the principal components for the model \", model_type.value)\n",
    "\n",
    "    for n in range(1, max_components + 1):\n",
    "        rmse, r2, pca_model = evalueate_principal_components_model(model_type, x_train, y_train, x_val, y_val, n)\n",
    "        r2_distance = abs(1 - r2)\n",
    "        score = alpha * r2_distance + beta * (rmse / rmse_baseline)\n",
    "        print(f\"{model_type.value} {n} - R²: {r2:.4f} - RMSE: {rmse:.4f} - Score: {score:.4f}\")\n",
    "        \n",
    "        # - The ideal R² is 1 ⇒ r2_distance = abs(1 - r2) measures how far you are from 1 (the smaller, the better).\n",
    "        # - The ideal RMSE is 0 ⇒ rmse / rmse_baseline measures the relative error with respect to a baseline (the smaller, the better).\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_n = n\n",
    "            best_pca_model = pca_model\n",
    "            best_rmse = rmse\n",
    "            best_r2 = r2\n",
    "\n",
    "    print(f\"Best {model_type.value} Model: n: {best_n} R²: {best_r2:.4f} RMSE: {best_rmse:.4f} Score: {best_score:.4f}\")\n",
    "    return best_rmse, best_r2, best_pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = processed_ds.copy()   \n",
    "y = df[PRICE]\n",
    "X = df.drop(columns=[PRICE]).select_dtypes(include=[\"int64\", \"float64\"]).drop(columns=[LOCATION, PRICE], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# 2. Dividir el dataset en train (60%), validation (20%) y test (20%)\n",
    "# Primero, separamos train (60%) y temp (40%)\n",
    "x_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Ahora separamos temp en val (20%) y test (20%)\n",
    "x_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance. The formula for each feature is:\n",
    "# X_scaled = (X - μ) / σ\n",
    "# μ = mean of the feature (calculated from the training set)\n",
    "# σ = standard deviation of the feature (calculated from the training set)\n",
    "# This transformation ensures that each feature has:\n",
    "# Mean = 0\n",
    "# Standard deviation = 1\n",
    "# It is important for algorithms like PCA because they are sensitive to the scale of the variables.\n",
    "# StandardScaler standardizes the features by removing the mean and scaling them to unit variance. Specifically, for each feature (column):\n",
    "scaler = StandardScaler()\n",
    "x_train_std = scaler.fit_transform(x_train)  # fit SOLO con train\n",
    "x_val_std   = scaler.transform(x_val)\n",
    "x_test_std  = scaler.transform(X_test)\n",
    "\n",
    "# Evaluación del modelo sin PCA\n",
    "pre_rmse, pre_r2_distance = lineal_regresion_model_apply(x_train_std, y_train, x_val_std, y_val)\n",
    "print(f\"Pre Regresion Model:  R²: {pre_r2_distance} RMSE: {pre_rmse:.4f}\")\n",
    "\n",
    "find_best_pca_model(x_train_std, y_train, x_val, y_val, pre_rmse, 0.7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8beb7f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresion without Reduction - R²: 0.8292 - RMSE: 3240671.3906\n",
      "\n",
      "\n",
      "Finding the principal components for the model  PCA\n",
      "PCA 1 - R²: -0.0067 - RMSE: 7867534.1017 - Score: 3.4344\n",
      "PCA 2 - R²: 0.6591 - RMSE: 4578148.7678 - Score: 1.7536\n",
      "PCA 3 - R²: 0.6548 - RMSE: 4607071.9221 - Score: 1.7668\n",
      "PCA 4 - R²: 0.6502 - RMSE: 4637735.1586 - Score: 1.7809\n",
      "PCA 5 - R²: 0.6540 - RMSE: 4612193.9501 - Score: 1.7692\n",
      "PCA 6 - R²: 0.6664 - RMSE: 4529302.2138 - Score: 1.7313\n",
      "PCA 7 - R²: 0.6565 - RMSE: 4595709.2089 - Score: 1.7616\n",
      "PCA 8 - R²: 0.6563 - RMSE: 4597443.0946 - Score: 1.7624\n",
      "PCA 9 - R²: 0.6688 - RMSE: 4512723.5048 - Score: 1.7237\n",
      "PCA 10 - R²: 0.6712 - RMSE: 4496622.5753 - Score: 1.7164\n",
      "PCA 11 - R²: 0.6705 - RMSE: 4501398.2036 - Score: 1.7186\n",
      "PCA 12 - R²: 0.6780 - RMSE: 4449740.9950 - Score: 1.6951\n",
      "PCA 13 - R²: 0.6803 - RMSE: 4433502.9986 - Score: 1.6878\n",
      "PCA 14 - R²: 0.6708 - RMSE: 4499342.7249 - Score: 1.7176\n",
      "PCA 15 - R²: 0.6778 - RMSE: 4451285.9163 - Score: 1.6958\n",
      "PCA 16 - R²: 0.7075 - RMSE: 4240962.6834 - Score: 1.6012\n",
      "PCA 17 - R²: 0.6845 - RMSE: 4404810.7209 - Score: 1.6748\n",
      "PCA 18 - R²: 0.6898 - RMSE: 4367298.6108 - Score: 1.6578\n",
      "PCA 19 - R²: 0.6679 - RMSE: 4519035.1510 - Score: 1.7266\n",
      "PCA 20 - R²: 0.6775 - RMSE: 4453039.0755 - Score: 1.6966\n",
      "PCA 21 - R²: 0.6905 - RMSE: 4362205.1804 - Score: 1.6556\n",
      "PCA 22 - R²: 0.6780 - RMSE: 4449390.3453 - Score: 1.6949\n",
      "PCA 23 - R²: 0.6967 - RMSE: 4318407.7235 - Score: 1.6359\n",
      "PCA 24 - R²: 0.6766 - RMSE: 4459126.0758 - Score: 1.6994\n",
      "PCA 25 - R²: 0.6753 - RMSE: 4468352.8553 - Score: 1.7036\n",
      "PCA 26 - R²: 0.7014 - RMSE: 4284625.2336 - Score: 1.6207\n",
      "PCA 27 - R²: 0.7063 - RMSE: 4249524.4693 - Score: 1.6050\n",
      "PCA 28 - R²: 0.6783 - RMSE: 4447709.3878 - Score: 1.6942\n",
      "PCA 29 - R²: 0.6889 - RMSE: 4373337.8121 - Score: 1.6606\n",
      "PCA 30 - R²: 0.6849 - RMSE: 4401624.0514 - Score: 1.6733\n",
      "PCA 31 - R²: 0.6959 - RMSE: 4324410.4283 - Score: 1.6386\n",
      "PCA 32 - R²: 0.6957 - RMSE: 4325855.0959 - Score: 1.6392\n",
      "PCA 33 - R²: 0.6831 - RMSE: 4414040.2759 - Score: 1.6789\n",
      "PCA 34 - R²: 0.6904 - RMSE: 4363305.6151 - Score: 1.6560\n",
      "PCA 35 - R²: 0.7004 - RMSE: 4292242.2897 - Score: 1.6241\n",
      "PCA 36 - R²: 0.7127 - RMSE: 4203027.2286 - Score: 1.5843\n",
      "PCA 37 - R²: 0.6947 - RMSE: 4332591.6818 - Score: 1.6422\n",
      "PCA 38 - R²: 0.6871 - RMSE: 4386573.3707 - Score: 1.6665\n",
      "PCA 39 - R²: 0.6775 - RMSE: 4453293.1963 - Score: 1.6967\n",
      "PCA 40 - R²: 0.6923 - RMSE: 4349622.7125 - Score: 1.6499\n",
      "PCA 41 - R²: 0.6930 - RMSE: 4344673.1628 - Score: 1.6477\n",
      "PCA 42 - R²: 0.7073 - RMSE: 4242535.5048 - Score: 1.6019\n",
      "PCA 43 - R²: 0.7003 - RMSE: 4292456.1519 - Score: 1.6242\n",
      "PCA 44 - R²: 0.7046 - RMSE: 4262120.7934 - Score: 1.6106\n",
      "PCA 45 - R²: 0.7046 - RMSE: 4261829.3058 - Score: 1.6105\n",
      "PCA 46 - R²: 0.7034 - RMSE: 4270393.5298 - Score: 1.6143\n",
      "PCA 47 - R²: 0.6864 - RMSE: 4390882.6666 - Score: 1.6685\n",
      "PCA 48 - R²: 0.7216 - RMSE: 4137678.3249 - Score: 1.5552\n",
      "PCA 49 - R²: 0.7032 - RMSE: 4272308.9598 - Score: 1.6152\n",
      "PCA 50 - R²: 0.6977 - RMSE: 4311714.4438 - Score: 1.6328\n",
      "PCA 51 - R²: 0.7194 - RMSE: 4153998.7540 - Score: 1.5625\n",
      "PCA 52 - R²: 0.7088 - RMSE: 4231403.4432 - Score: 1.5969\n",
      "PCA 53 - R²: 0.7086 - RMSE: 4232864.9268 - Score: 1.5976\n",
      "PCA 54 - R²: 0.7228 - RMSE: 4128253.9792 - Score: 1.5511\n",
      "PCA 55 - R²: 0.7026 - RMSE: 4276236.4651 - Score: 1.6169\n",
      "PCA 56 - R²: 0.7187 - RMSE: 4159252.1967 - Score: 1.5648\n",
      "PCA 57 - R²: 0.7141 - RMSE: 4192606.5601 - Score: 1.5796\n",
      "PCA 58 - R²: 0.7006 - RMSE: 4290489.2370 - Score: 1.6233\n",
      "PCA 59 - R²: 0.7023 - RMSE: 4278155.2651 - Score: 1.6178\n",
      "PCA 60 - R²: 0.7056 - RMSE: 4254841.8738 - Score: 1.6074\n",
      "PCA 61 - R²: 0.7024 - RMSE: 4277552.8761 - Score: 1.6175\n",
      "PCA 62 - R²: 0.7051 - RMSE: 4258111.1080 - Score: 1.6088\n",
      "PCA 63 - R²: 0.7176 - RMSE: 4167346.4636 - Score: 1.5684\n",
      "PCA 64 - R²: 0.7138 - RMSE: 4194855.3724 - Score: 1.5806\n",
      "PCA 65 - R²: 0.7213 - RMSE: 4139590.4642 - Score: 1.5561\n",
      "PCA 66 - R²: 0.6999 - RMSE: 4295439.8979 - Score: 1.6255\n",
      "PCA 67 - R²: 0.7048 - RMSE: 4260179.0197 - Score: 1.6098\n",
      "PCA 68 - R²: 0.7249 - RMSE: 4112559.4118 - Score: 1.5441\n",
      "PCA 69 - R²: 0.7112 - RMSE: 4214281.7138 - Score: 1.5893\n",
      "PCA 70 - R²: 0.7149 - RMSE: 4186907.6211 - Score: 1.5771\n",
      "PCA 71 - R²: 0.7215 - RMSE: 4138294.2583 - Score: 1.5555\n",
      "PCA 72 - R²: 0.7171 - RMSE: 4170394.5611 - Score: 1.5697\n",
      "PCA 73 - R²: 0.7234 - RMSE: 4124073.9345 - Score: 1.5492\n",
      "PCA 74 - R²: 0.7305 - RMSE: 4070504.9628 - Score: 1.5255\n",
      "PCA 75 - R²: 0.7409 - RMSE: 3991227.1326 - Score: 1.4907\n",
      "PCA 76 - R²: 0.7294 - RMSE: 4079005.8789 - Score: 1.5293\n",
      "PCA 77 - R²: 0.7209 - RMSE: 4142714.0158 - Score: 1.5575\n",
      "PCA 78 - R²: 0.7287 - RMSE: 4084137.8557 - Score: 1.5315\n",
      "PCA 79 - R²: 0.7267 - RMSE: 4099143.4640 - Score: 1.5382\n",
      "PCA 80 - R²: 0.7144 - RMSE: 4190339.5683 - Score: 1.5786\n",
      "PCA 81 - R²: 0.7271 - RMSE: 4096327.1474 - Score: 1.5369\n",
      "PCA 82 - R²: 0.7210 - RMSE: 4142078.9276 - Score: 1.5572\n",
      "PCA 83 - R²: 0.7292 - RMSE: 4080398.8475 - Score: 1.5299\n",
      "PCA 84 - R²: 0.7360 - RMSE: 4029282.7800 - Score: 1.5074\n",
      "PCA 85 - R²: 0.7134 - RMSE: 4198242.3722 - Score: 1.5821\n",
      "PCA 86 - R²: 0.7340 - RMSE: 4044378.7188 - Score: 1.5140\n",
      "PCA 87 - R²: 0.7264 - RMSE: 4101731.6906 - Score: 1.5393\n",
      "PCA 88 - R²: 0.7214 - RMSE: 4138882.1301 - Score: 1.5558\n",
      "PCA 89 - R²: 0.7020 - RMSE: 4280825.0883 - Score: 1.6190\n",
      "PCA 90 - R²: 0.7022 - RMSE: 4279117.0659 - Score: 1.6182\n",
      "PCA 91 - R²: 0.7271 - RMSE: 4096531.6383 - Score: 1.5370\n",
      "PCA 92 - R²: 0.7433 - RMSE: 3972849.4899 - Score: 1.4826\n",
      "PCA 93 - R²: 0.7270 - RMSE: 4096751.0263 - Score: 1.5371\n",
      "PCA 94 - R²: 0.7340 - RMSE: 4044080.1543 - Score: 1.5139\n",
      "PCA 95 - R²: 0.7397 - RMSE: 4000617.2354 - Score: 1.4948\n",
      "PCA 96 - R²: 0.7247 - RMSE: 4113980.9217 - Score: 1.5447\n",
      "PCA 97 - R²: 0.7344 - RMSE: 4040898.4644 - Score: 1.5125\n",
      "PCA 98 - R²: 0.7280 - RMSE: 4089665.3255 - Score: 1.5340\n",
      "PCA 99 - R²: 0.7226 - RMSE: 4129909.1759 - Score: 1.5518\n",
      "PCA 100 - R²: 0.7363 - RMSE: 4026808.7791 - Score: 1.5063\n",
      "PCA 101 - R²: 0.7393 - RMSE: 4003874.2509 - Score: 1.4962\n",
      "PCA 102 - R²: 0.7211 - RMSE: 4141034.4380 - Score: 1.5567\n",
      "PCA 103 - R²: 0.7408 - RMSE: 3991970.7855 - Score: 1.4910\n",
      "PCA 104 - R²: 0.7338 - RMSE: 4045610.6694 - Score: 1.5146\n",
      "PCA 105 - R²: 0.7336 - RMSE: 4047270.5927 - Score: 1.5153\n",
      "PCA 106 - R²: 0.7405 - RMSE: 3994542.8161 - Score: 1.4921\n",
      "PCA 107 - R²: 0.7316 - RMSE: 4062503.6205 - Score: 1.5220\n",
      "PCA 108 - R²: 0.7356 - RMSE: 4032046.1729 - Score: 1.5086\n",
      "PCA 109 - R²: 0.7225 - RMSE: 4130581.2430 - Score: 1.5521\n",
      "PCA 110 - R²: 0.7332 - RMSE: 4049978.4861 - Score: 1.5165\n",
      "PCA 111 - R²: 0.7386 - RMSE: 4008737.7710 - Score: 1.4984\n",
      "PCA 112 - R²: 0.7371 - RMSE: 4020961.1479 - Score: 1.5037\n",
      "PCA 113 - R²: 0.7381 - RMSE: 4012605.8941 - Score: 1.5001\n",
      "PCA 114 - R²: 0.7221 - RMSE: 4133632.4588 - Score: 1.5534\n",
      "PCA 115 - R²: 0.7251 - RMSE: 4111468.6226 - Score: 1.5436\n",
      "PCA 116 - R²: 0.7513 - RMSE: 3910561.6398 - Score: 1.4554\n",
      "PCA 117 - R²: 0.7349 - RMSE: 4037611.2610 - Score: 1.5110\n",
      "PCA 118 - R²: 0.7465 - RMSE: 3948152.2277 - Score: 1.4718\n",
      "PCA 119 - R²: 0.7343 - RMSE: 4042294.7555 - Score: 1.5131\n",
      "PCA 120 - R²: 0.7449 - RMSE: 3960587.8268 - Score: 1.4773\n",
      "PCA 121 - R²: 0.7474 - RMSE: 3941312.2286 - Score: 1.4688\n",
      "PCA 122 - R²: 0.7470 - RMSE: 3944513.6960 - Score: 1.4702\n",
      "PCA 123 - R²: 0.7521 - RMSE: 3904519.5247 - Score: 1.4528\n",
      "PCA 124 - R²: 0.7412 - RMSE: 3988821.5287 - Score: 1.4896\n",
      "PCA 125 - R²: 0.7517 - RMSE: 3907677.2419 - Score: 1.4542\n",
      "PCA 126 - R²: 0.7437 - RMSE: 3970004.8727 - Score: 1.4814\n",
      "PCA 127 - R²: 0.7515 - RMSE: 3909107.8639 - Score: 1.4548\n",
      "PCA 128 - R²: 0.7473 - RMSE: 3941916.8935 - Score: 1.4691\n",
      "PCA 129 - R²: 0.7402 - RMSE: 3997183.7397 - Score: 1.4933\n",
      "PCA 130 - R²: 0.7499 - RMSE: 3921539.4283 - Score: 1.4602\n",
      "PCA 131 - R²: 0.7542 - RMSE: 3887716.9889 - Score: 1.4455\n",
      "PCA 132 - R²: 0.7442 - RMSE: 3966012.3187 - Score: 1.4796\n",
      "PCA 133 - R²: 0.7417 - RMSE: 3984913.3547 - Score: 1.4879\n",
      "PCA 134 - R²: 0.7495 - RMSE: 3924859.0808 - Score: 1.4617\n",
      "PCA 135 - R²: 0.7425 - RMSE: 3978811.6829 - Score: 1.4852\n",
      "PCA 136 - R²: 0.7480 - RMSE: 3936139.2209 - Score: 1.4666\n",
      "PCA 137 - R²: 0.7454 - RMSE: 3956303.6771 - Score: 1.4754\n",
      "PCA 138 - R²: 0.7449 - RMSE: 3960265.1012 - Score: 1.4771\n",
      "PCA 139 - R²: 0.7533 - RMSE: 3894480.8953 - Score: 1.4484\n",
      "PCA 140 - R²: 0.7567 - RMSE: 3868067.1067 - Score: 1.4369\n",
      "PCA 141 - R²: 0.7454 - RMSE: 3956780.0747 - Score: 1.4756\n",
      "PCA 142 - R²: 0.7590 - RMSE: 3849143.0276 - Score: 1.4287\n",
      "PCA 143 - R²: 0.7601 - RMSE: 3840871.3378 - Score: 1.4251\n",
      "PCA 144 - R²: 0.7545 - RMSE: 3885050.1697 - Score: 1.4443\n",
      "PCA 145 - R²: 0.7591 - RMSE: 3848658.9263 - Score: 1.4285\n",
      "PCA 146 - R²: 0.7412 - RMSE: 3988867.2999 - Score: 1.4896\n",
      "PCA 147 - R²: 0.7511 - RMSE: 3912122.4519 - Score: 1.4561\n",
      "PCA 148 - R²: 0.7389 - RMSE: 4006922.4051 - Score: 1.4976\n",
      "PCA 149 - R²: 0.7595 - RMSE: 3845768.7549 - Score: 1.4273\n",
      "PCA 150 - R²: 0.7470 - RMSE: 3944245.4214 - Score: 1.4701\n",
      "PCA 151 - R²: 0.7474 - RMSE: 3940980.1730 - Score: 1.4687\n",
      "PCA 152 - R²: 0.7498 - RMSE: 3922652.7722 - Score: 1.4607\n",
      "PCA 153 - R²: 0.7609 - RMSE: 3833933.3000 - Score: 1.4221\n",
      "PCA 154 - R²: 0.7617 - RMSE: 3827509.3663 - Score: 1.4193\n",
      "PCA 155 - R²: 0.7547 - RMSE: 3883820.7552 - Score: 1.4438\n",
      "PCA 156 - R²: 0.7557 - RMSE: 3875690.0334 - Score: 1.4402\n",
      "PCA 157 - R²: 0.7716 - RMSE: 3747596.2513 - Score: 1.3848\n",
      "PCA 158 - R²: 0.7606 - RMSE: 3836673.4502 - Score: 1.4233\n",
      "PCA 159 - R²: 0.7627 - RMSE: 3820011.3876 - Score: 1.4161\n",
      "PCA 160 - R²: 0.7623 - RMSE: 3823424.8173 - Score: 1.4176\n",
      "PCA 161 - R²: 0.7628 - RMSE: 3818874.4998 - Score: 1.4156\n",
      "PCA 162 - R²: 0.7611 - RMSE: 3832687.7410 - Score: 1.4216\n",
      "PCA 163 - R²: 0.7562 - RMSE: 3872147.2460 - Score: 1.4387\n",
      "PCA 164 - R²: 0.7644 - RMSE: 3806295.3018 - Score: 1.4102\n",
      "PCA 165 - R²: 0.7583 - RMSE: 3854899.3396 - Score: 1.4312\n",
      "PCA 166 - R²: 0.7601 - RMSE: 3840345.1965 - Score: 1.4249\n",
      "PCA 167 - R²: 0.7602 - RMSE: 3839879.6898 - Score: 1.4247\n",
      "PCA 168 - R²: 0.7582 - RMSE: 3856068.1823 - Score: 1.4317\n",
      "PCA 169 - R²: 0.7527 - RMSE: 3899882.8209 - Score: 1.4508\n",
      "PCA 170 - R²: 0.7530 - RMSE: 3897083.7598 - Score: 1.4495\n",
      "PCA 171 - R²: 0.7618 - RMSE: 3826880.2997 - Score: 1.4191\n",
      "PCA 172 - R²: 0.7593 - RMSE: 3847143.4700 - Score: 1.4278\n",
      "PCA 173 - R²: 0.7636 - RMSE: 3812435.4825 - Score: 1.4128\n",
      "PCA 174 - R²: 0.7623 - RMSE: 3823149.7615 - Score: 1.4175\n",
      "PCA 175 - R²: 0.7661 - RMSE: 3792643.6142 - Score: 1.4043\n",
      "PCA 176 - R²: 0.7646 - RMSE: 3804267.8961 - Score: 1.4093\n",
      "PCA 177 - R²: 0.7628 - RMSE: 3819139.3315 - Score: 1.4157\n",
      "PCA 178 - R²: 0.7586 - RMSE: 3852640.0312 - Score: 1.4302\n",
      "PCA 179 - R²: 0.7652 - RMSE: 3799284.9096 - Score: 1.4071\n",
      "PCA 180 - R²: 0.7649 - RMSE: 3801704.5306 - Score: 1.4082\n",
      "PCA 181 - R²: 0.7594 - RMSE: 3846226.4757 - Score: 1.4275\n",
      "PCA 182 - R²: 0.7669 - RMSE: 3786271.6457 - Score: 1.4015\n",
      "PCA 183 - R²: 0.7640 - RMSE: 3809077.6270 - Score: 1.4114\n",
      "PCA 184 - R²: 0.7641 - RMSE: 3808877.8328 - Score: 1.4113\n",
      "PCA 185 - R²: 0.7651 - RMSE: 3800095.0200 - Score: 1.4075\n",
      "PCA 186 - R²: 0.7655 - RMSE: 3796927.6069 - Score: 1.4061\n",
      "PCA 187 - R²: 0.7648 - RMSE: 3802492.2757 - Score: 1.4085\n",
      "PCA 188 - R²: 0.7650 - RMSE: 3801642.4617 - Score: 1.4081\n",
      "PCA 189 - R²: 0.7646 - RMSE: 3804753.2878 - Score: 1.4095\n",
      "PCA 190 - R²: 0.7651 - RMSE: 3800784.2132 - Score: 1.4078\n",
      "PCA 191 - R²: 0.7650 - RMSE: 3801074.7103 - Score: 1.4079\n",
      "PCA 192 - R²: 0.7628 - RMSE: 3819156.9327 - Score: 1.4157\n",
      "PCA 193 - R²: 0.7583 - RMSE: 3854952.5816 - Score: 1.4312\n",
      "PCA 194 - R²: 0.7587 - RMSE: 3851782.1723 - Score: 1.4299\n",
      "PCA 195 - R²: 0.7717 - RMSE: 3746981.9228 - Score: 1.3846\n",
      "PCA 196 - R²: 0.8456 - RMSE: 3080928.2247 - Score: 1.1051\n",
      "PCA 197 - R²: 0.8469 - RMSE: 3067877.7908 - Score: 1.0997\n",
      "PCA 198 - R²: 0.8475 - RMSE: 3061870.0187 - Score: 1.0973\n",
      "PCA 199 - R²: 0.8477 - RMSE: 3060662.6694 - Score: 1.0968\n",
      "PCA 200 - R²: 0.8477 - RMSE: 3060309.4726 - Score: 1.0967\n",
      "PCA 201 - R²: 0.8461 - RMSE: 3076496.3077 - Score: 1.1033\n",
      "PCA 202 - R²: 0.8461 - RMSE: 3076161.0138 - Score: 1.1031\n",
      "PCA 203 - R²: 0.8464 - RMSE: 3072719.4841 - Score: 1.1017\n",
      "PCA 204 - R²: 0.8451 - RMSE: 3086546.6350 - Score: 1.1074\n",
      "PCA 205 - R²: 0.8441 - RMSE: 3096545.5214 - Score: 1.1115\n",
      "PCA 206 - R²: 0.8429 - RMSE: 3107705.6399 - Score: 1.1160\n",
      "PCA 207 - R²: 0.8427 - RMSE: 3109712.0898 - Score: 1.1169\n",
      "PCA 208 - R²: 0.8429 - RMSE: 3108112.8725 - Score: 1.1162\n",
      "PCA 209 - R²: 0.8417 - RMSE: 3119915.1812 - Score: 1.1210\n",
      "PCA 210 - R²: 0.8420 - RMSE: 3117189.0565 - Score: 1.1199\n",
      "PCA 211 - R²: 0.8419 - RMSE: 3117717.9914 - Score: 1.1201\n",
      "PCA 212 - R²: 0.8405 - RMSE: 3132016.1013 - Score: 1.1260\n",
      "PCA 213 - R²: 0.8406 - RMSE: 3130554.8138 - Score: 1.1254\n",
      "PCA 214 - R²: 0.8409 - RMSE: 3127410.5261 - Score: 1.1241\n",
      "PCA 215 - R²: 0.8358 - RMSE: 3177829.6738 - Score: 1.1448\n",
      "PCA 216 - R²: 0.8340 - RMSE: 3195170.7875 - Score: 1.1520\n",
      "PCA 217 - R²: 0.8349 - RMSE: 3186406.6853 - Score: 1.1484\n",
      "PCA 218 - R²: 0.8343 - RMSE: 3191909.5579 - Score: 1.1506\n",
      "PCA 219 - R²: 0.8342 - RMSE: 3192983.1499 - Score: 1.1511\n",
      "PCA 220 - R²: 0.8316 - RMSE: 3218316.3013 - Score: 1.1615\n",
      "PCA 221 - R²: 0.8244 - RMSE: 3285684.4812 - Score: 1.1895\n",
      "PCA 222 - R²: 0.8235 - RMSE: 3294441.8787 - Score: 1.1931\n",
      "PCA 223 - R²: 0.8229 - RMSE: 3300328.3739 - Score: 1.1956\n",
      "PCA 224 - R²: 0.8235 - RMSE: 3294449.7181 - Score: 1.1931\n",
      "PCA 225 - R²: 0.8261 - RMSE: 3270124.3859 - Score: 1.1830\n",
      "PCA 226 - R²: 0.8280 - RMSE: 3252522.8082 - Score: 1.1757\n",
      "PCA 227 - R²: 0.8281 - RMSE: 3251029.7905 - Score: 1.1751\n",
      "PCA 228 - R²: 0.8245 - RMSE: 3284879.4834 - Score: 1.1891\n",
      "PCA 229 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 230 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 231 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 232 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 233 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 234 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 235 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 236 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 237 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 238 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 239 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 240 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 241 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 242 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 243 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 244 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 245 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 246 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 247 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 248 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 249 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 250 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 251 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 252 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 253 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 254 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 255 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 256 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 257 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 258 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 259 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 260 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 261 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 262 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 263 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 264 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 265 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 266 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 267 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 268 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 269 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 270 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 271 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 272 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 273 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 274 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 275 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 276 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 277 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 278 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 279 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 280 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "PCA 281 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "Best PCA Model: n: 200 R²: 0.8477 RMSE: 3060309.4726 Score: 1.0967\n",
      "\n",
      "Finding the principal components for the model  SVD\n",
      "SVD 1 - R²: -0.0067 - RMSE: 7867534.1017 - Score: 3.4344\n",
      "SVD 2 - R²: 0.6598 - RMSE: 4573754.6271 - Score: 1.7516\n",
      "SVD 3 - R²: 0.6542 - RMSE: 4611086.6386 - Score: 1.7687\n",
      "SVD 4 - R²: 0.6556 - RMSE: 4601901.8844 - Score: 1.7645\n",
      "SVD 5 - R²: 0.6560 - RMSE: 4599163.9034 - Score: 1.7632\n",
      "SVD 6 - R²: 0.6637 - RMSE: 4547569.9480 - Score: 1.7396\n",
      "SVD 7 - R²: 0.6537 - RMSE: 4614168.9949 - Score: 1.7701\n",
      "SVD 8 - R²: 0.6625 - RMSE: 4555616.2562 - Score: 1.7433\n",
      "SVD 9 - R²: 0.6663 - RMSE: 4529778.3222 - Score: 1.7315\n",
      "SVD 10 - R²: 0.6695 - RMSE: 4508062.5124 - Score: 1.7216\n",
      "SVD 11 - R²: 0.6738 - RMSE: 4478597.7532 - Score: 1.7082\n",
      "SVD 12 - R²: 0.6689 - RMSE: 4512179.8028 - Score: 1.7235\n",
      "SVD 13 - R²: 0.6750 - RMSE: 4470002.4226 - Score: 1.7043\n",
      "SVD 14 - R²: 0.6705 - RMSE: 4501243.0379 - Score: 1.7185\n",
      "SVD 15 - R²: 0.6889 - RMSE: 4373338.6176 - Score: 1.6606\n",
      "SVD 16 - R²: 0.6841 - RMSE: 4407244.6047 - Score: 1.6759\n",
      "SVD 17 - R²: 0.6775 - RMSE: 4452788.9677 - Score: 1.6965\n",
      "SVD 18 - R²: 0.6877 - RMSE: 4382413.9974 - Score: 1.6647\n",
      "SVD 19 - R²: 0.6845 - RMSE: 4404827.1883 - Score: 1.6748\n",
      "SVD 20 - R²: 0.6859 - RMSE: 4394584.6743 - Score: 1.6702\n",
      "SVD 21 - R²: 0.6866 - RMSE: 4389942.7795 - Score: 1.6681\n",
      "SVD 22 - R²: 0.6662 - RMSE: 4530353.4116 - Score: 1.7318\n",
      "SVD 23 - R²: 0.6867 - RMSE: 4389275.7962 - Score: 1.6678\n",
      "SVD 24 - R²: 0.6845 - RMSE: 4404805.6578 - Score: 1.6748\n",
      "SVD 25 - R²: 0.6945 - RMSE: 4334473.5443 - Score: 1.6431\n",
      "SVD 26 - R²: 0.6890 - RMSE: 4373016.0959 - Score: 1.6604\n",
      "SVD 27 - R²: 0.6810 - RMSE: 4428696.3348 - Score: 1.6856\n",
      "SVD 28 - R²: 0.6752 - RMSE: 4468942.1860 - Score: 1.7038\n",
      "SVD 29 - R²: 0.7014 - RMSE: 4284594.2783 - Score: 1.6207\n",
      "SVD 30 - R²: 0.6816 - RMSE: 4424963.6698 - Score: 1.6839\n",
      "SVD 31 - R²: 0.6891 - RMSE: 4372357.0989 - Score: 1.6601\n",
      "SVD 32 - R²: 0.6922 - RMSE: 4350455.4708 - Score: 1.6503\n",
      "SVD 33 - R²: 0.6877 - RMSE: 4381762.9626 - Score: 1.6644\n",
      "SVD 34 - R²: 0.6817 - RMSE: 4424299.6522 - Score: 1.6836\n",
      "SVD 35 - R²: 0.7025 - RMSE: 4277041.7393 - Score: 1.6173\n",
      "SVD 36 - R²: 0.6950 - RMSE: 4330929.4661 - Score: 1.6415\n",
      "SVD 37 - R²: 0.6847 - RMSE: 4402755.6558 - Score: 1.6738\n",
      "SVD 38 - R²: 0.6981 - RMSE: 4308534.6642 - Score: 1.6314\n",
      "SVD 39 - R²: 0.6715 - RMSE: 4494079.6770 - Score: 1.7152\n",
      "SVD 40 - R²: 0.6749 - RMSE: 4470875.2880 - Score: 1.7047\n",
      "SVD 41 - R²: 0.7050 - RMSE: 4259127.6881 - Score: 1.6093\n",
      "SVD 42 - R²: 0.7103 - RMSE: 4220637.1476 - Score: 1.5921\n",
      "SVD 43 - R²: 0.6970 - RMSE: 4316626.6574 - Score: 1.6351\n",
      "SVD 44 - R²: 0.6757 - RMSE: 4465198.4475 - Score: 1.7021\n",
      "SVD 45 - R²: 0.7056 - RMSE: 4254683.7193 - Score: 1.6073\n",
      "SVD 46 - R²: 0.7146 - RMSE: 4189090.2719 - Score: 1.5781\n",
      "SVD 47 - R²: 0.7255 - RMSE: 4108540.5290 - Score: 1.5423\n",
      "SVD 48 - R²: 0.6816 - RMSE: 4424925.7112 - Score: 1.6839\n",
      "SVD 49 - R²: 0.6816 - RMSE: 4424803.2784 - Score: 1.6838\n",
      "SVD 50 - R²: 0.6979 - RMSE: 4310087.5394 - Score: 1.6321\n",
      "SVD 51 - R²: 0.7137 - RMSE: 4195915.3894 - Score: 1.5811\n",
      "SVD 52 - R²: 0.7046 - RMSE: 4262151.7290 - Score: 1.6106\n",
      "SVD 53 - R²: 0.7186 - RMSE: 4160024.3621 - Score: 1.5651\n",
      "SVD 54 - R²: 0.7173 - RMSE: 4168942.1809 - Score: 1.5691\n",
      "SVD 55 - R²: 0.7127 - RMSE: 4202854.2356 - Score: 1.5842\n",
      "SVD 56 - R²: 0.7042 - RMSE: 4264405.6229 - Score: 1.6117\n",
      "SVD 57 - R²: 0.7263 - RMSE: 4102498.9680 - Score: 1.5397\n",
      "SVD 58 - R²: 0.7214 - RMSE: 4138834.8547 - Score: 1.5557\n",
      "SVD 59 - R²: 0.7221 - RMSE: 4133794.9874 - Score: 1.5535\n",
      "SVD 60 - R²: 0.7066 - RMSE: 4247470.3310 - Score: 1.6041\n",
      "SVD 61 - R²: 0.7011 - RMSE: 4287144.7534 - Score: 1.6218\n",
      "SVD 62 - R²: 0.7213 - RMSE: 4139455.0891 - Score: 1.5560\n",
      "SVD 63 - R²: 0.7346 - RMSE: 4039334.0945 - Score: 1.5118\n",
      "SVD 64 - R²: 0.7288 - RMSE: 4083557.0671 - Score: 1.5313\n",
      "SVD 65 - R²: 0.7044 - RMSE: 4263566.0060 - Score: 1.6113\n",
      "SVD 66 - R²: 0.7162 - RMSE: 4177179.2661 - Score: 1.5728\n",
      "SVD 67 - R²: 0.7342 - RMSE: 4042959.2916 - Score: 1.5134\n",
      "SVD 68 - R²: 0.7201 - RMSE: 4148337.4221 - Score: 1.5600\n",
      "SVD 69 - R²: 0.7226 - RMSE: 4129895.1424 - Score: 1.5518\n",
      "SVD 70 - R²: 0.7136 - RMSE: 4196600.6306 - Score: 1.5814\n",
      "SVD 71 - R²: 0.7068 - RMSE: 4245839.2909 - Score: 1.6034\n",
      "SVD 72 - R²: 0.7158 - RMSE: 4179961.5262 - Score: 1.5740\n",
      "SVD 73 - R²: 0.7246 - RMSE: 4115343.4797 - Score: 1.5453\n",
      "SVD 74 - R²: 0.7209 - RMSE: 4142631.4950 - Score: 1.5574\n",
      "SVD 75 - R²: 0.7239 - RMSE: 4120332.2930 - Score: 1.5475\n",
      "SVD 76 - R²: 0.7181 - RMSE: 4163242.2903 - Score: 1.5666\n",
      "SVD 77 - R²: 0.7298 - RMSE: 4076360.3113 - Score: 1.5281\n",
      "SVD 78 - R²: 0.7197 - RMSE: 4151844.7683 - Score: 1.5615\n",
      "SVD 79 - R²: 0.7286 - RMSE: 4085428.8989 - Score: 1.5321\n",
      "SVD 80 - R²: 0.7162 - RMSE: 4177013.0438 - Score: 1.5727\n",
      "SVD 81 - R²: 0.7230 - RMSE: 4126689.6839 - Score: 1.5504\n",
      "SVD 82 - R²: 0.7224 - RMSE: 4131757.7036 - Score: 1.5526\n",
      "SVD 83 - R²: 0.7269 - RMSE: 4097933.3565 - Score: 1.5376\n",
      "SVD 84 - R²: 0.7254 - RMSE: 4109438.5476 - Score: 1.5427\n",
      "SVD 85 - R²: 0.7348 - RMSE: 4037931.0304 - Score: 1.5112\n",
      "SVD 86 - R²: 0.7253 - RMSE: 4109555.0759 - Score: 1.5428\n",
      "SVD 87 - R²: 0.7234 - RMSE: 4124175.7635 - Score: 1.5492\n",
      "SVD 88 - R²: 0.7315 - RMSE: 4062870.3667 - Score: 1.5222\n",
      "SVD 89 - R²: 0.7301 - RMSE: 4073756.3357 - Score: 1.5270\n",
      "SVD 90 - R²: 0.7324 - RMSE: 4056668.8515 - Score: 1.5194\n",
      "SVD 91 - R²: 0.7340 - RMSE: 4044005.5269 - Score: 1.5139\n",
      "SVD 92 - R²: 0.7287 - RMSE: 4084419.1615 - Score: 1.5317\n",
      "SVD 93 - R²: 0.7306 - RMSE: 4070034.5154 - Score: 1.5253\n",
      "SVD 94 - R²: 0.7350 - RMSE: 4036673.6550 - Score: 1.5106\n",
      "SVD 95 - R²: 0.7267 - RMSE: 4099620.8465 - Score: 1.5384\n",
      "SVD 96 - R²: 0.7316 - RMSE: 4062209.3695 - Score: 1.5219\n",
      "SVD 97 - R²: 0.7491 - RMSE: 3928152.5559 - Score: 1.4631\n",
      "SVD 98 - R²: 0.7292 - RMSE: 4080374.2476 - Score: 1.5299\n",
      "SVD 99 - R²: 0.7211 - RMSE: 4141356.9430 - Score: 1.5569\n",
      "SVD 100 - R²: 0.7375 - RMSE: 4017649.5347 - Score: 1.5023\n",
      "SVD 101 - R²: 0.7358 - RMSE: 4030498.6005 - Score: 1.5079\n",
      "SVD 102 - R²: 0.7201 - RMSE: 4148817.9996 - Score: 1.5602\n",
      "SVD 103 - R²: 0.7252 - RMSE: 4110305.8968 - Score: 1.5431\n",
      "SVD 104 - R²: 0.7188 - RMSE: 4158225.3187 - Score: 1.5643\n",
      "SVD 105 - R²: 0.7494 - RMSE: 3925111.5805 - Score: 1.4618\n",
      "SVD 106 - R²: 0.7315 - RMSE: 4063291.1642 - Score: 1.5224\n",
      "SVD 107 - R²: 0.7457 - RMSE: 3954333.0644 - Score: 1.4745\n",
      "SVD 108 - R²: 0.7379 - RMSE: 4014767.6128 - Score: 1.5010\n",
      "SVD 109 - R²: 0.7263 - RMSE: 4102201.1720 - Score: 1.5395\n",
      "SVD 110 - R²: 0.7465 - RMSE: 3948289.9399 - Score: 1.4719\n",
      "SVD 111 - R²: 0.7331 - RMSE: 4051287.4855 - Score: 1.5171\n",
      "SVD 112 - R²: 0.7440 - RMSE: 3967372.5474 - Score: 1.4802\n",
      "SVD 113 - R²: 0.7426 - RMSE: 3978606.7645 - Score: 1.4851\n",
      "SVD 114 - R²: 0.7303 - RMSE: 4072642.7693 - Score: 1.5265\n",
      "SVD 115 - R²: 0.7410 - RMSE: 3990743.3677 - Score: 1.4905\n",
      "SVD 116 - R²: 0.7183 - RMSE: 4161887.4286 - Score: 1.5660\n",
      "SVD 117 - R²: 0.7588 - RMSE: 3850814.6103 - Score: 1.4294\n",
      "SVD 118 - R²: 0.7484 - RMSE: 3933130.6938 - Score: 1.4653\n",
      "SVD 119 - R²: 0.7473 - RMSE: 3942082.0990 - Score: 1.4692\n",
      "SVD 120 - R²: 0.7396 - RMSE: 4001737.5429 - Score: 1.4953\n",
      "SVD 121 - R²: 0.7470 - RMSE: 3944296.8910 - Score: 1.4701\n",
      "SVD 122 - R²: 0.7490 - RMSE: 3928703.7475 - Score: 1.4633\n",
      "SVD 123 - R²: 0.7403 - RMSE: 3996061.0990 - Score: 1.4928\n",
      "SVD 124 - R²: 0.7434 - RMSE: 3971880.8757 - Score: 1.4822\n",
      "SVD 125 - R²: 0.7550 - RMSE: 3881316.8338 - Score: 1.4427\n",
      "SVD 126 - R²: 0.7466 - RMSE: 3947639.0039 - Score: 1.4716\n",
      "SVD 127 - R²: 0.7392 - RMSE: 4004654.7623 - Score: 1.4966\n",
      "SVD 128 - R²: 0.7488 - RMSE: 3929879.1659 - Score: 1.4638\n",
      "SVD 129 - R²: 0.7511 - RMSE: 3911864.5020 - Score: 1.4560\n",
      "SVD 130 - R²: 0.7401 - RMSE: 3997843.6465 - Score: 1.4936\n",
      "SVD 131 - R²: 0.7540 - RMSE: 3888955.1444 - Score: 1.4460\n",
      "SVD 132 - R²: 0.7509 - RMSE: 3914016.6498 - Score: 1.4569\n",
      "SVD 133 - R²: 0.7460 - RMSE: 3952268.9510 - Score: 1.4736\n",
      "SVD 134 - R²: 0.7518 - RMSE: 3906342.5609 - Score: 1.4536\n",
      "SVD 135 - R²: 0.7505 - RMSE: 3916874.1175 - Score: 1.4582\n",
      "SVD 136 - R²: 0.7562 - RMSE: 3871690.3939 - Score: 1.4385\n",
      "SVD 137 - R²: 0.7555 - RMSE: 3877385.2655 - Score: 1.4410\n",
      "SVD 138 - R²: 0.7456 - RMSE: 3955017.6446 - Score: 1.4748\n",
      "SVD 139 - R²: 0.7498 - RMSE: 3922529.1951 - Score: 1.4606\n",
      "SVD 140 - R²: 0.7517 - RMSE: 3907072.3301 - Score: 1.4539\n",
      "SVD 141 - R²: 0.7555 - RMSE: 3877437.1496 - Score: 1.4410\n",
      "SVD 142 - R²: 0.7635 - RMSE: 3813113.9067 - Score: 1.4131\n",
      "SVD 143 - R²: 0.7469 - RMSE: 3944675.1871 - Score: 1.4703\n",
      "SVD 144 - R²: 0.7507 - RMSE: 3915038.5667 - Score: 1.4574\n",
      "SVD 145 - R²: 0.7591 - RMSE: 3848596.9445 - Score: 1.4285\n",
      "SVD 146 - R²: 0.7553 - RMSE: 3878933.5220 - Score: 1.4417\n",
      "SVD 147 - R²: 0.7573 - RMSE: 3863440.3320 - Score: 1.4349\n",
      "SVD 148 - R²: 0.7563 - RMSE: 3870805.7938 - Score: 1.4381\n",
      "SVD 149 - R²: 0.7496 - RMSE: 3923714.1932 - Score: 1.4612\n",
      "SVD 150 - R²: 0.7395 - RMSE: 4002454.7926 - Score: 1.4956\n",
      "SVD 151 - R²: 0.7575 - RMSE: 3861521.9605 - Score: 1.4341\n",
      "SVD 152 - R²: 0.7484 - RMSE: 3933428.4116 - Score: 1.4654\n",
      "SVD 153 - R²: 0.7601 - RMSE: 3840374.5945 - Score: 1.4249\n",
      "SVD 154 - R²: 0.7498 - RMSE: 3922541.6230 - Score: 1.4606\n",
      "SVD 155 - R²: 0.7573 - RMSE: 3863323.4725 - Score: 1.4349\n",
      "SVD 156 - R²: 0.7620 - RMSE: 3825465.9011 - Score: 1.4185\n",
      "SVD 157 - R²: 0.7619 - RMSE: 3826339.4900 - Score: 1.4188\n",
      "SVD 158 - R²: 0.7598 - RMSE: 3843349.5198 - Score: 1.4262\n",
      "SVD 159 - R²: 0.7583 - RMSE: 3855199.8699 - Score: 1.4313\n",
      "SVD 160 - R²: 0.7570 - RMSE: 3865652.8709 - Score: 1.4359\n",
      "SVD 161 - R²: 0.7597 - RMSE: 3844246.2596 - Score: 1.4266\n",
      "SVD 162 - R²: 0.7601 - RMSE: 3841036.5550 - Score: 1.4252\n",
      "SVD 163 - R²: 0.7572 - RMSE: 3863638.6165 - Score: 1.4350\n",
      "SVD 164 - R²: 0.7588 - RMSE: 3851479.5010 - Score: 1.4297\n",
      "SVD 165 - R²: 0.7564 - RMSE: 3870070.1215 - Score: 1.4378\n",
      "SVD 166 - R²: 0.7578 - RMSE: 3858967.4428 - Score: 1.4330\n",
      "SVD 167 - R²: 0.7616 - RMSE: 3828729.3153 - Score: 1.4199\n",
      "SVD 168 - R²: 0.7620 - RMSE: 3825218.8160 - Score: 1.4183\n",
      "SVD 169 - R²: 0.7615 - RMSE: 3829748.5770 - Score: 1.4203\n",
      "SVD 170 - R²: 0.7600 - RMSE: 3841762.0439 - Score: 1.4255\n",
      "SVD 171 - R²: 0.7641 - RMSE: 3808691.8141 - Score: 1.4112\n",
      "SVD 172 - R²: 0.7627 - RMSE: 3819733.8362 - Score: 1.4160\n",
      "SVD 173 - R²: 0.7599 - RMSE: 3842233.0141 - Score: 1.4257\n",
      "SVD 174 - R²: 0.7637 - RMSE: 3811596.0477 - Score: 1.4125\n",
      "SVD 175 - R²: 0.7639 - RMSE: 3809967.3629 - Score: 1.4117\n",
      "SVD 176 - R²: 0.7643 - RMSE: 3806795.5254 - Score: 1.4104\n",
      "SVD 177 - R²: 0.7636 - RMSE: 3812188.4715 - Score: 1.4127\n",
      "SVD 178 - R²: 0.7652 - RMSE: 3799530.7304 - Score: 1.4072\n",
      "SVD 179 - R²: 0.7633 - RMSE: 3815192.8361 - Score: 1.4140\n",
      "SVD 180 - R²: 0.7654 - RMSE: 3797934.9030 - Score: 1.4065\n",
      "SVD 181 - R²: 0.7651 - RMSE: 3800125.7462 - Score: 1.4075\n",
      "SVD 182 - R²: 0.7650 - RMSE: 3801405.5349 - Score: 1.4080\n",
      "SVD 183 - R²: 0.7646 - RMSE: 3804590.9140 - Score: 1.4094\n",
      "SVD 184 - R²: 0.7653 - RMSE: 3798755.1493 - Score: 1.4069\n",
      "SVD 185 - R²: 0.7651 - RMSE: 3800854.0613 - Score: 1.4078\n",
      "SVD 186 - R²: 0.7658 - RMSE: 3794755.6774 - Score: 1.4052\n",
      "SVD 187 - R²: 0.7642 - RMSE: 3807387.6075 - Score: 1.4106\n",
      "SVD 188 - R²: 0.7653 - RMSE: 3798541.7080 - Score: 1.4068\n",
      "SVD 189 - R²: 0.7650 - RMSE: 3801303.1230 - Score: 1.4080\n",
      "SVD 190 - R²: 0.7651 - RMSE: 3800757.4052 - Score: 1.4078\n",
      "SVD 191 - R²: 0.7650 - RMSE: 3801074.7203 - Score: 1.4079\n",
      "SVD 192 - R²: 0.7628 - RMSE: 3819156.9401 - Score: 1.4157\n",
      "SVD 193 - R²: 0.7583 - RMSE: 3854952.5921 - Score: 1.4312\n",
      "SVD 194 - R²: 0.7587 - RMSE: 3851782.1638 - Score: 1.4299\n",
      "SVD 195 - R²: 0.7717 - RMSE: 3746982.4300 - Score: 1.3846\n",
      "SVD 196 - R²: 0.8456 - RMSE: 3080930.2786 - Score: 1.1051\n",
      "SVD 197 - R²: 0.8469 - RMSE: 3067878.9614 - Score: 1.0997\n",
      "SVD 198 - R²: 0.8475 - RMSE: 3061872.4945 - Score: 1.0973\n",
      "SVD 199 - R²: 0.8477 - RMSE: 3060515.4958 - Score: 1.0967\n",
      "SVD 200 - R²: 0.8477 - RMSE: 3060329.5213 - Score: 1.0967\n",
      "SVD 201 - R²: 0.8461 - RMSE: 3076163.8493 - Score: 1.1031\n",
      "SVD 202 - R²: 0.8461 - RMSE: 3076105.7054 - Score: 1.1031\n",
      "SVD 203 - R²: 0.8464 - RMSE: 3072969.6765 - Score: 1.1018\n",
      "SVD 204 - R²: 0.8450 - RMSE: 3087142.9474 - Score: 1.1076\n",
      "SVD 205 - R²: 0.8441 - RMSE: 3096149.6892 - Score: 1.1113\n",
      "SVD 206 - R²: 0.8427 - RMSE: 3110305.8752 - Score: 1.1171\n",
      "SVD 207 - R²: 0.8427 - RMSE: 3109868.9173 - Score: 1.1169\n",
      "SVD 208 - R²: 0.8428 - RMSE: 3108589.5915 - Score: 1.1164\n",
      "SVD 209 - R²: 0.8416 - RMSE: 3121112.4416 - Score: 1.1215\n",
      "SVD 210 - R²: 0.8419 - RMSE: 3117489.4089 - Score: 1.1200\n",
      "SVD 211 - R²: 0.8419 - RMSE: 3117757.0431 - Score: 1.1202\n",
      "SVD 212 - R²: 0.8405 - RMSE: 3131922.9655 - Score: 1.1260\n",
      "SVD 213 - R²: 0.8406 - RMSE: 3130334.0170 - Score: 1.1253\n",
      "SVD 214 - R²: 0.8410 - RMSE: 3127224.2643 - Score: 1.1240\n",
      "SVD 215 - R²: 0.8358 - RMSE: 3177789.5375 - Score: 1.1448\n",
      "SVD 216 - R²: 0.8340 - RMSE: 3195196.6567 - Score: 1.1520\n",
      "SVD 217 - R²: 0.8349 - RMSE: 3186328.5931 - Score: 1.1483\n",
      "SVD 218 - R²: 0.8343 - RMSE: 3191908.1100 - Score: 1.1506\n",
      "SVD 219 - R²: 0.8342 - RMSE: 3192983.1499 - Score: 1.1511\n",
      "SVD 220 - R²: 0.8316 - RMSE: 3218316.3013 - Score: 1.1615\n",
      "SVD 221 - R²: 0.8244 - RMSE: 3285684.4812 - Score: 1.1895\n",
      "SVD 222 - R²: 0.8235 - RMSE: 3294441.8787 - Score: 1.1931\n",
      "SVD 223 - R²: 0.8229 - RMSE: 3300328.3739 - Score: 1.1956\n",
      "SVD 224 - R²: 0.8235 - RMSE: 3294449.7181 - Score: 1.1931\n",
      "SVD 225 - R²: 0.8261 - RMSE: 3270124.3859 - Score: 1.1830\n",
      "SVD 226 - R²: 0.8280 - RMSE: 3252522.8082 - Score: 1.1757\n",
      "SVD 227 - R²: 0.8281 - RMSE: 3251029.7905 - Score: 1.1751\n",
      "SVD 228 - R²: 0.8245 - RMSE: 3284879.4834 - Score: 1.1891\n",
      "SVD 229 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 230 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 231 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 232 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 233 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 234 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 235 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 236 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 237 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 238 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 239 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 240 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 241 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 242 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 243 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 244 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 245 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 246 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 247 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 248 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 249 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 250 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 251 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 252 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 253 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 254 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 255 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 256 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 257 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 258 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 259 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 260 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 261 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 262 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 263 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 264 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 265 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 266 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 267 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 268 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 269 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 270 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 271 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 272 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 273 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 274 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 275 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 276 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 277 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 278 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 279 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 280 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "SVD 281 - R²: 0.8292 - RMSE: 3240671.3906 - Score: 1.1708\n",
      "Best SVD Model: n: 200 R²: 0.8477 RMSE: 3060329.5213 Score: 1.0967\n",
      "\n",
      "Finding the principal components for the model  LDA\n",
      "LDA 1 - R²: -0.0053 - RMSE: 7862070.8903 - Score: 3.4313\n",
      "LDA 2 - R²: -0.0057 - RMSE: 7863800.6616 - Score: 3.4323\n",
      "LDA 3 - R²: -0.0059 - RMSE: 7864675.0594 - Score: 3.4328\n",
      "LDA 4 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 5 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 6 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 7 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 8 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 9 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 10 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 11 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 12 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 13 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 14 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 15 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 16 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 17 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 18 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 19 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 20 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 21 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 22 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 23 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 24 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 25 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 26 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 27 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 28 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 29 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 30 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 31 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 32 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 33 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 34 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 35 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 36 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 37 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 38 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 39 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 40 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 41 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 42 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 43 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 44 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 45 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 46 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 47 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 48 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 49 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 50 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 51 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 52 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 53 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 54 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 55 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 56 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 57 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 58 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 59 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 60 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 61 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 62 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 63 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 64 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 65 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 66 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 67 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 68 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 69 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 70 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 71 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 72 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 73 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 74 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 75 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 76 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 77 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 78 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 79 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 80 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 81 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 82 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 83 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 84 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 85 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 86 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 87 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 88 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 89 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 90 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 91 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 92 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 93 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 94 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 95 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 96 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 97 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 98 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 99 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 100 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 101 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 102 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 103 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 104 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 105 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 106 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 107 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 108 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 109 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 110 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 111 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 112 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 113 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 114 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 115 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 116 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 117 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 118 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 119 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 120 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 121 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 122 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 123 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 124 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 125 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 126 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 127 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 128 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 129 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 130 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 131 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 132 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 133 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 134 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 135 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 136 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 137 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 138 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 139 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 140 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 141 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 142 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 143 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 144 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 145 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 146 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 147 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 148 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 149 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 150 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 151 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 152 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 153 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 154 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 155 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 156 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 157 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 158 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 159 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 160 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 161 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 162 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 163 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 164 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 165 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 166 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 167 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 168 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 169 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 170 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 171 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 172 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 173 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 174 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 175 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 176 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 177 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 178 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 179 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 180 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 181 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 182 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 183 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 184 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 185 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 186 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 187 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 188 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 189 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 190 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 191 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 192 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 193 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 194 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 195 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 196 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 197 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 198 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 199 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 200 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 201 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 202 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 203 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 204 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 205 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 206 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 207 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 208 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 209 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 210 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 211 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 212 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 213 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 214 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 215 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 216 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 217 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 218 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 219 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 220 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 221 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 222 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 223 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 224 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 225 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 226 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 227 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 228 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 229 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 230 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 231 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 232 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 233 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 234 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 235 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 236 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 237 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 238 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 239 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 240 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 241 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 242 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 243 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 244 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 245 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 246 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 247 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 248 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 249 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 250 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 251 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 252 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 253 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 254 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 255 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 256 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 257 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 258 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 259 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 260 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 261 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 262 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 263 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 264 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 265 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 266 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 267 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 268 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 269 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 270 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 271 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 272 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 273 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 274 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 275 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 276 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 277 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 278 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 279 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 280 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "LDA 281 - R²: -0.0064 - RMSE: 7866566.1617 - Score: 3.4339\n",
      "Best LDA Model: n: 1 R²: -0.0053 RMSE: 7862070.8903 Score: 3.4313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Cargar el dataset\n",
    "diabetes = load_diabetes()\n",
    "df = processed_ds.copy()\n",
    "y = df[PRICE]\n",
    "X = df.drop(columns=[PRICE])\n",
    "\n",
    "\n",
    "# Dividir el dataset en train (60%), validation (20%) y test (20%)\n",
    "x_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "x_train_std = scaler.fit_transform(x_train)\n",
    "x_val_std = scaler.transform(x_val)\n",
    "x_test_std = scaler.transform(x_test)\n",
    "\n",
    "# Evaluar modelo sin PCA\n",
    "pre_rmse, pre_r2 = lineal_regresion_model_apply(x_train_std, y_train, x_val_std, y_val)\n",
    "print(f\"Regresion without Reduction - R²: {pre_r2:.4f} - RMSE: {pre_rmse:.4f}\\n\")\n",
    "\n",
    "# Buscar mejor modelo PCA\n",
    "best_pca_rmse, best_pca_r2, best_pca_model = find_best_principal_components_model(ModelType.PCA, x_train_std, y_train, x_val_std, y_val, rmse_baseline=pre_rmse)\n",
    "best_svd_rmse, best_svd_r2, best_svd_model = find_best_principal_components_model(ModelType.SVD, x_train_std, y_train, x_val_std, y_val, rmse_baseline=pre_rmse)\n",
    "best_lda_rmse, best_lda_r2, best_lda_model = find_best_principal_components_model(ModelType.LDA, x_train_std, y_train, x_val_std, y_val, rmse_baseline=pre_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5dd5f",
   "metadata": {},
   "source": [
    "## Export Processed Dataset\n",
    "\n",
    "Export the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ds.to_csv(\"../../../datasets/raw/hyderabad_house_price_processed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
